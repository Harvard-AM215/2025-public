{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Applied Mathematics 115: Mathematical Modeling  \n",
                "---\n",
                "*2024 Fall / Full Term*\n",
                "\n",
                "**Meeting Time:**  \n",
                "Tuesday 10:30 AM - 11:45 AM  \n",
                "Thursday 10:30 AM - 11:45 AM\n",
                "\u003cbr\u003e\n",
                "\u003cbr\u003e\n",
                "\n",
                "**Instructors:**  \n",
                "*Michael P. Brenner* (Pierce Hall 313) brenner@seas.harvard.edu  \n",
                "Ignacio Becker iebecker@g.harvard.edu\n",
                "\n",
                "**Teaching Fellows:**  \n",
                "- Katya Ivshina\n",
                "- Elle Weeks\n",
                "- Livia Guttieres\n",
                "- Erik Wang\n",
                "- Sarah Martinson\n",
                "- Ryan Krueger\n",
                "- Alp Sunol\n",
                "- Zeyuan Hu\n",
                "- Qian-Ze Zhu\n",
                "- Laura Zichi\n",
                "- Francesco Mottes\n",
                "\n",
                "\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Section 1 : Probability, MLE and Mosteller\n",
                "\n",
                "Please see the notebooks for lectures 1 and 2 for accompanying introductory materials for this section, and for useful definitions you may need to refer to when answering the following questions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Probability\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q1: Probability Properties\n",
                "\n",
                "Consider a fair six-sided die. Use Python code to calculate and verify the probability properties\n",
                "1. $P(A^C) = 1- P(A)$\n",
                "2. $P(A\\cup B) = P(A) + P(B) - P(A\\cap B),$\n",
                "\n",
                "where $A$ is the event of rolling an even number and $B$ is the event of rolling a number $ \u003e 4$.\n",
                "\n",
                "**Note:** you may have already proved these properties theoretically in lectures or if you followed the exercises in the lecture notebook. Here, we want you to use a coded example to verify that these properties hold in this case.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q1) ###\n",
                "## SAMPLE RESPONSE:\n",
                "\n",
                "# Sample space for 6-sided die\n",
                "S = {1, 2, 3, 4, 5, 6}\n",
                "\n",
                "# Definitions\n",
                "A = {2, 4, 6}  # Even numbers\n",
                "Ac = {1, 3, 5} # Odd numbers\n",
                "B = {5, 6}     # \u003e 4\n",
                "\n",
                "# Calculate probabilities\n",
                "# your code here\n",
                "P_A = ...\n",
                "P_A_complement = ...\n",
                "\n",
                "P_B = ...\n",
                "P_A_intersect_B = ...\n",
                "P_A_union_B = ..."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q2: Conditional Probabilities\n",
                "\n",
                "A standard deck of 52 cards is shuffled. Two cards are drawn sequentially *without replacement*. Write a Python function to calculate the conditional probability $P(A∣B)$, where $A$ is the event that the first card is a heart and $B$ is the event that the second card is red."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q2) ###\n",
                "## SAMPLE RESPONSE\n",
                "\n",
                "def conditional_probability(lenA, lenB):\n",
                "    # your code here\n",
                "    ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "hearts = ... # Number of hearts (lenA)\n",
                "reds = ...   # Number of red cards (lenB)\n",
                "\n",
                "print(f\"P(A|B) = {conditional_probability(hearts, reds)}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q3: Binomial Distribution—Visualizations\n",
                "\n",
                "Modify the provided `plot_binomial` function from the lecture notes to visualise the distribution for $p=0.8$ and $n=15$. If you run the same code again, is the distribution the same? Why or why not?\n",
                "\n",
                "What do you observe about the distribution as $p$ increases from 0.5 to 0.8? What about when we increase $n$ from 10 or 15 to 100 or even 1000?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "## COPIED CODE FROM LECTURE NOTEBOOK FOR plot_binomial FUNCTION\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from ipywidgets import interact\n",
                "\n",
                "def plot_binomial(p=0.5, n=10):\n",
                "    print(f\"Probability (p): {p}\")\n",
                "    print(f\"Number of trials (n): {n}\")\n",
                "    print(\"--------------------------------------------------\")\n",
                "    bernoulli = np.random.binomial(1, p, n)\n",
                "    unique, counts = np.unique(bernoulli, return_counts=True)\n",
                "    plt.bar(unique, counts)\n",
                "    plt.xticks([0, 1])\n",
                "    plt.xlabel('Value')\n",
                "    plt.ylabel('Frequency')\n",
                "    plt.title('Binomial Distribution')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code h\n",
                "interact(plot_binomial, p=(p_ini, p_end, step), n=(n_ini,n_end, steps));"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q4: Poisson Distribution\n",
                "\n",
                "A website receives 7 visits per minute on average. Use the Poisson distribution to calculate and visualise the probability distribution of the number of visits per minute. Then, compute the probability of receiving *exactly 5* visits in a minute.\n",
                "\n",
                "Hint: Use the code provided in the lecture notebook for visualisation, and use the definitions from the notebook to define your Poisson function!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_poisson(lam=5, sample_size=1000):\n",
                "    print(f\"Lambda (λ): {lam}\")\n",
                "    print(f\"Size: {sample_size}\")\n",
                "    print(\"--------------------------------------------------\")\n",
                "    poisson = np.random.poisson(lam, sample_size)\n",
                "    unique, counts = np.unique(poisson, return_counts=True)\n",
                "    plt.bar(unique, counts)\n",
                "    plt.xlabel('Value')\n",
                "    plt.ylabel('Frequency')\n",
                "    plt.title('Poisson Distribution')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from math import exp, factorial\n",
                "\n",
                "# Define Poisson distribution\n",
                "def poisson_pmf(lam, x):\n",
                "    return ...\n",
                "    \n",
                "# P(5 visits in a min)\n",
                "lambda_visits = 7 # NB to note that lambda is a reserved var in Python\n",
                "prob_5_visits = poisson_pmf(lambda_visits, 5)\n",
                "print(f\"Probability of exactly 5 visits in a minute: {prob_5_visits:.4f}\")\n",
                "\n",
                "# Visualisation for lambda = 7 and sample size = 1000\n",
                "plot_poisson(lam=7, sample_size=1000)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Maximum Likelihood Estimation"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q5: MLE for a Biased Die\n",
                "\n",
                "You have a die that may or may not be biased, and you want to estimate the probability of rolling a *specific* number. Assume that all other numbers have an equal probability, and let $p$ be the probability of rolling your chosen number. You are free to assign $p$.\n",
                "\n",
                "Given a series of rolls, estimate $p$ using MLE. Implement the estimation analytically and compare it with a numerical estimation. Do this by generating a synthetic dataset of rolls, then using `sp.optimize.minimize` for the numeric estimation (as done in the lecture notebooks). Plot the negative log-likelihood function and mark the analytic and numeric solutions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import scipy as sp\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Generate synthetic data\n",
                "n_rolls = 100\n",
                "p_true = 0.2\n",
                "data = (np.random.uniform(size=n_rolls) \u003c p_true).astype(int)\n",
                "\n",
                "# Analytic MLE for p\n",
                "p_analytic = np.mean(data)\n",
                "print(f'Analytic MLE for p = {p_analytic:.5f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Negative Log-Likelihood function\n",
                "\n",
                "def NLL_die(p, data):\n",
                "    n_specific = np.sum(data)\n",
                "    n_not = len(data) - n_specific\n",
                "    return ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initial guess\n",
                "p_init = np.random.uniform()\n",
                "\n",
                "# Numerical MLE\n",
                "epsilon = 1e-5\n",
                "opt_results = sp.optimize.minimize(lambda p: NLL_die(p, data),\n",
                "                                      p_init,\n",
                "                                      bounds=[(0. + epsilon, 1. - epsilon)])\n",
                "\n",
                "p_numeric = opt_results.x.squeeze()\n",
                "print(f'Numerical MLE for p = {p_numeric:.5f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot --\u003e following code from lecture notebook\n",
                "\n",
                "ps = np.linspace(0.001, 0.999, 300)\n",
                "lls = [NLL_die(p, data) for p in ps]\n",
                "\n",
                "plt.plot(ps, lls, 'r-')\n",
                "plt.plot(p_analytic, NLL_die(p_analytic, data), 's', markersize=10, label=f'Analytic MLE: p={p_analytic:.3f}')\n",
                "plt.plot(p_numeric, NLL_die(p_numeric, data), '*', markersize=10, label=f'Numeric MLE: p={p_numeric:.3f}')\n",
                "\n",
                "plt.xlabel('p')\n",
                "plt.ylabel('Negative log-likelihood')\n",
                "plt.grid(alpha=.2)\n",
                "plt.legend()\n",
                "plt.show();"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q6: MLE for a Mixture of Gaussians\n",
                "\n",
                "Consider a dataset generated by sampling from a mixture of two Gaussians. Each data point is sampled either from a Gaussian $\\mu_1 = 2, ~ \\sigma^2_1 = 1$ or from a Gaussian $\\mu_2 = 5, ~ \\sigma^2_2 = 1.$ Let the probability of sampling from the first Gaussian be $p = 0.6$.\n",
                "\n",
                "Estimate parameters $\\mu_1, ~ \\mu_2, ~ p$ using MLE. Implement both analytic and numerical solutions. Compare the true and estimated parameters and the true and estimated distributions. Can you see the individual Gaussians in both the true data and the estimation?\n",
                "\n",
                "**Hint 1**: Generate synthetic data from the mixture of Gaussians (as we've done before in the lecture notebook).\n",
                "\n",
                "**Hint 2**: Derive the log-likelihood function and implement its *gradient*."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.stats import norm\n",
                "\n",
                "# Generate synthetic data\n",
                "n_samples = 200\n",
                "p_true = 0.6\n",
                "mu1_true, mu2_true = 2, 5\n",
                "sigma1_true, sigma2_true = 1, 1\n",
                "data = np.concatenate([\n",
                "    np.random.normal(...),\n",
                "    np.random.normal(...)\n",
                "])\n",
                "\n",
                "print(f'True values: mu1 = {mu1_true}, mu2 = {mu2_true}, p = {p_true}')\n",
                "\n",
                "\n",
                "# Negative Log-Likelihood function for Gaussian Mixture\n",
                "def NLL_gaussian_mixture(params, data):\n",
                "    mu1, mu2, p = params\n",
                "    likelihoods = ...\n",
                "    return -np.sum(np.log(likelihoods))\n",
                "\n",
                "\n",
                "# Initial param guesses\n",
                "params_init = [np.mean(data) - 1, np.mean(data) + 1, 0.5]\n",
                "\n",
                "\n",
                "# Numerical MLE\n",
                "opt_results = sp.optimize.minimize(lambda params: NLL_gaussian_mixture(params, data),\n",
                "                                      params_init,\n",
                "                                      bounds=[(-np.inf, np.inf), (-np.inf, np.inf), (0.01, 0.99)])\n",
                "mu1_numeric, mu2_numeric, p_numeric = opt_results.x\n",
                "\n",
                "\n",
                "print(f'Estimated values: mu1 = {mu1_numeric:.3f}, mu2 = {mu2_numeric:.3f}, p = {p_numeric:.3f}')\n",
                "\n",
                "# Plot\n",
                "\n",
                "plt.hist(data, bins=30, density=True, alpha=0.6, color='g', label='Data Histogram')\n",
                "x = np.linspace(min(data), max(data), 1000)\n",
                "\n",
                "plt.plot(x, p_numeric * norm.pdf(x, mu1_numeric, sigma1_true) + (1 - p_numeric) * norm.pdf(x, mu2_numeric, sigma2_true), 'r-', lw=2, label='Estimated Mixture')\n",
                "\n",
                "plt.xlabel('Value')\n",
                "plt.ylabel('Density')\n",
                "plt.legend()\n",
                "plt.show();"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The Mosteller Model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Q7: Mosteller\n",
                "\n",
                "The Mosteller model assumes each game in the World Series can be treated as a Bernoulli trial, where the better team has a constant probability $p\u003e0.5$ of winning each game. We saw in class (and in the lecture notebook) that, in Mosteller's case, the MLE for $p$ comes out to about $0.6551$.\n",
                "\n",
                "What does this value for $p$ mean? Based on this, how effective is the best-of-seven structure used in the World Series at ensuring that the better team wins the series overall? Discuss the implications of the estimate $ p \\approx 0.6551$ for the fairness and reliability of the World Series format and outcome."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
